## Easy and Efficient Transformer

<div align='right' ><font size="1"><b><a href="./README.md">ENGLISH README</a></b> </font></div>


<div  align="center"> <img src="./doc/image/EETblueLOGO.png" width = "600" height = "180" alt="EET" align=center /></div>
</br>

<p align="center">
    <a href="https://github.com/NetEase-FuXi/EET/blob/main/LICENSE">
        <img alt="GitHub license" src="./doc/image/license.svg">
    </a>
    <a href="https://github.com/NetEase-FuXi/EET/tree/main/example/python">
        <img alt="GitHub release" src="./doc/image/example.svg">
    </a>
    <a href="https://github.com/NetEase-FuXi/EET/releases">
        <img alt="release" src="./doc/image/release.svg">
    </a>
</p>

EETï¼ˆEasy But Efficient Transformerï¼‰æ˜¯ä¸€æ¬¾é’ˆå¯¹Transformer-basedå¤§æ¨¡å‹å’Œé•¿åºåˆ—åœºæ™¯çš„é«˜æ€§èƒ½pytorchæ¨ç†æ’ä»¶ã€‚

## åŠŸèƒ½ç‰¹æ€§
* **æ–°ç‰¹æ€§**ğŸ”¥: ç°å·²æ”¯æŒBaichuanã€LLaMAç­‰LLMsã€‚
* é«˜æ€§èƒ½ï¼šè®¾è®¡é«˜åº¦ä¼˜åŒ–çš„CUDAå†…æ ¸ã€‚
* çµæ´»ï¼šæä¾›äº†åŒ…æ‹¬op apiã€model apiå’Œpipelinesåº”å¯¹ä¸åŒçš„éœ€æ±‚ 
* æ˜“äºä½¿ç”¨ï¼š å‡ è¡Œä»£ç å³å¯å®Œæˆã€‚ 
* é€‚é…ä¸»æµaiæ¡†æ¶ï¼ŒåŒ…æ‹¬fairseqå’Œtransformersã€‚
* bertæ¨¡å‹æ•´ä½“æ€§èƒ½åŠ é€Ÿ1.2xåˆ°7.xå€ï¼Œgptæ¨¡å‹æ•´ä½“æ€§èƒ½åŠ é€Ÿ2.xåˆ°7.xå€ã€‚

---

- [Easy and Efficient Transformer](#easy-and-efficient-transformer)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [æ”¯æŒæ¨¡å‹](#æ”¯æŒæ¨¡å‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  - [ç¯å¢ƒ](#ç¯å¢ƒ)
  - [å®‰è£…](#å®‰è£…)
    - [æºç å®‰è£…](#æºç å®‰è£…)
    - [dockeré•œåƒå®‰è£…](#dockeré•œåƒå®‰è£…)
  - [è¿è¡Œ](#è¿è¡Œ)
    - [operators API](#operators-api)
    - [Model API](#model-api)
    - [pipelinesæ–¹å¼](#pipelinesæ–¹å¼)
- [æ€§èƒ½](#æ€§èƒ½)
- [Cite Us](#cite-us)
- [è”ç³»æˆ‘ä»¬](#è”ç³»æˆ‘ä»¬)

## æ”¯æŒæ¨¡å‹

| Model | Since version | 
|-------|-------------|
| GPT2 | 0.0.1 beta |
| Bert | 0.0.1 beta | 
| Roberta | 1.0 | 
| Albert | 1.0 |
| Vit | 1.0 |
| Clip | 1.0 |
| Distilbert | 1.0 |
| Baichuan | 2.0 |
| LLaMA | 2.0 |

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒ

* cuda:>=10.1 
* python:>=3.7 
* gcc:>= 7.4.0 
* torch:>=1.5.0 
* numpy:>=1.19.1 
* fairseq
* transformers

ä¸Šè¿°ç¯å¢ƒæ˜¯æœ€ä½é…ç½®ï¼Œæœ€å¥½æ˜¯ä½¿ç”¨è¾ƒæ–°çš„ç‰ˆæœ¬ã€‚

æ¨èä½¿ç”¨nvcr.io/nvidia/é•œåƒ

### å®‰è£…

æ¨èä½¿ç”¨dockerå®‰è£…

#### æºç å®‰è£…
å¦‚æœä»æºä»£ç å®‰è£…ï¼Œåˆ™éœ€è¦å®‰è£…å¿…è¦çš„[environment](#environment)ã€‚ç„¶åï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œã€‚ 
```bash
$ git clone https://github.com/NetEase-FuXi/EET.git
$ pip install .
```

#### dockeré•œåƒå®‰è£…

æ¨èä½¿ç”¨nvcr.io/nvidia/pytorch:21.12-py3ç­‰ç³»åˆ—é•œåƒï¼Œä¹Ÿå¯ä½¿ç”¨æä¾›çš„Dockerfileæ–‡ä»¶

```bash
$ git clone https://github.com/NetEase-FuXi/EET.git
$ docker build -t eet_docker:0.1 .
$ nvidia-docker run -it --net=host -v /your/project/directory/:/root/workspace  eet_docker:0.1 bash
```

æ­¤æ—¶ï¼ŒEETåŠå…¶æ‰€éœ€çš„ç¯å¢ƒå‡å·²å®‰è£…åœ¨dockerä¸­ã€‚

### è¿è¡Œ
æˆ‘ä»¬æä¾›ä¸‰ç§è¿è¡Œæ–¹å¼ï¼š

#### operators API

æˆ‘ä»¬æä¾›äº†Transformeræ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ç®—å­ï¼Œä¾‹å¦‚multiheadattentionï¼Œlayernormï¼ŒFFNç­‰ï¼Œæ‚¨å¯ä»¥ç»“åˆä¸åŒçš„ç®—å­æ¥æ„å»ºä¸åŒçš„æ¨¡å‹ç»“æ„
- operators API è¡¨

    | operators API | Remarks | 
    |-------|-------------|
    | masked_multi_head_attention | GPT2 self_attention |
    | cross_multi_head_attention | cross_attention | 
    | multi_head_attention | Bert self_attention | 
    | ffn | FeedForwardNetwork |
    | embedding | transformers & fairseq |
    | layernorm | nn.LayerNorm |

- ä½¿ç”¨æ–¹å¼

    ä½ å¯ä»¥å‚è€ƒä¸Šé¢åˆ—å‡ºçš„op APIæ¥æ„é€ è‡ªå·±çš„æ¨¡å‹ç»“æ„ï¼Œåªè¦ä¿®æ”¹[python/eet](./python/eet)ä¸‹çš„æ–‡ä»¶å°±å¯ä»¥äº†ã€‚


#### Model API

EETæä¾›äº†éå¸¸å‹å¥½çš„modelçº§api([python/eet](./python/eet))ï¼Œé€‚é…fairseqå’Œtransformersæ¨¡å‹åªéœ€è¦å‡ è¡Œä»£ç ã€‚
ç”¨æ³•éå¸¸ç®€å•ï¼Œä»…ä¸‰è¡Œä»£ç å°±å¯ä»¥å®Œå…¨é€‚é…transformersã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒEETçš„GPTæ¨¡å‹åªæ”¯æŒå·¦è¾¹æ‰“paddingï¼Œå…¶ä»–æ¨¡å‹æ”¯æŒå³è¾¹æ‰“paddingã€‚
    
<b>EET and fairseq class comparison table</b>
| EET | fairseq| Remarks | 
|-------|-------------|-------------| 
| EETTransformerDecoder | TransformerDecoder |  |
| EETTransformerDecoderLayer | TransformerDecoderLayer |  |
| EETTransformerAttention | MultiheadAttention |  |
| EETTransformerFeedforward | TransformerDecoderLayer | fusion of multiple small operators |
| EETTransformerEmbedding | Embedding + PositionalEmbedding |  |
| EETTransformerLayerNorm | nn.LayerNorm |  |

<b>EET and transformers class comparison table</b>
| EET | transformers| Remarks | 
|---------------|-----------------|----| 
| EETBertModel | BertModel |  |
| EETBertEncoder | BertEncoder |  |
| EETBertEncoderLayer | BertLayer |  |
| EETBertAttention | BertAttention |  |
| EETBertFeedforward | BertIntermediate + BertOutput |  |
| EETBertEmbedding | BertEmbeddings |  |
| EETGPT2Model | GPT2Model |  |
| EETGPT2Decoder | GPT2Model | transformers has no GPT2Decoder |
| EETGPT2DecoderLayer | Block |  |
| EETGPT2Attention | Attention | |
| EETGPT2Feedforward | MLP |  |
| EETGPT2Embedding | nn.Embedding |  |
| EETLayerNorm | nn.LayerNorm |  |

ä¸ºäº†æ›´å¥½åœ°é€‚é…transformersï¼Œæˆ‘ä»¬åœ¨transformersçš„åŸºç¡€ä¸Šæ‰©å±•äº†æ”¯æŒæ¨¡å‹çš„api,ä¾‹å¦‚ï¼Œå¯¹äºbertæ¨¡å‹ï¼Œæˆ‘ä»¬å¢åŠ äº†ä»¥ä¸‹apiæ¥æ”¯æŒä¸åŒçš„ä»»åŠ¡ã€‚     

| EET | transformers| Remarks | 
|---------------|-----------------|----| 
| EETBertForPreTraining | BertForPreTraining | No |
| EETBertLMHeadModel | BertLMHeadModel | No |
| EETBertForMaskedLM | BertForMaskedLM | No |
| EETBertForNextSentencePrediction | BertForNextSentencePrediction | No |
| EETBertForSequenceClassification | BertForSequenceClassification | No |
| EETBertForMultipleChoice | BertForMultipleChoice | No |
| EETBertForTokenClassification | BertForTokenClassification | No |
| EETBertForQuestionAnswering | BertForQuestionAnswering | No |

å…¶ä»–æ¨¡å‹ä¹Ÿéƒ½æä¾›äº†ç›¸å…³apiã€‚

æ¨ç†æ–¹æ³•ï¼š

<div  align="left"> <img src="./doc/image/use_bert.png" width = "850" height = "325" alt="useofbert"/></div>

ä½ ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›model apiå®ç°è‡ªå·±çš„ç‰¹å®šä»»åŠ¡ï¼Œä¸‹é¢ä»¥fill-maskä»»åŠ¡ä¸ºä¾‹ï¼š

```python
from eet import EETRobertaForMaskedLM
from transformers import RobertaTokenizer
input = ["My <mask> is Sarah and I live in London"]
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
eet_roberta_model = EETRobertaForMaskedLM.from_pretrained(model_path,max_batch = max_batch_size,data_type = data_type)
# first step: tokenize
model_inputs = tokenizer(input,return_tensors = 'pt')
masked_index = torch.nonzero(model_inputs['input_ids'][0] == tokenizer.mask_token_id, as_tuple=False).squeeze(-1)
# second step: predict
prediction_scores = eet_roberta_model(model_inputs['input_ids'].cuda(),attention_mask = model_inputs['attention_mask'])
# third step: argmax
predicted_index = torch.argmax(prediction_scores.logits[0, masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)
```

è¯·å‚è€ƒ [example/python/models](example/python/models/).

#### pipelinesæ–¹å¼

EETæä¾›äº†ç°æˆçš„pipelinesæ–¹å¼ï¼ŒåŸºäºEETæ”¯æŒçš„ä¸åŒæ¨¡å‹ç»“æ„ï¼Œæä¾›ä¸åŒä»»åŠ¡çš„pipelineä½¿ç”¨æ–¹æ¡ˆã€‚

ä½¿ç”¨æ–¹å¼éå¸¸ç®€å•ï¼š

```python
import torch
from eet import pipeline
max_batch_size = 1
model_path = 'roberta-base'
data_type = torch.float16
input = ["My <mask> is Sarah and I live in London"]
nlp = pipeline("fill-mask",model = model_path,data_type = data_type,max_batch_size = max_batch_size)
out = nlp(input)
```

å…·ä½“æ”¯æŒï¼š

| Task | Since version | 
|-------|-------------|
| text-classification | 1.0 |
| token-classification | 1.0 | 
| question-answering | 1.0 | 
| fill-mask | 1.0 |
| text-generation | 1.0 |
| image-classification | 1.0 |
| zero_shot_image_classification | 1.0 |

åç»­éšç€EETæ”¯æŒçš„æ¨¡å‹è¶Šæ¥è¶Šå¤šï¼Œæ”¯æŒçš„pipelineä»»åŠ¡ä¹Ÿå°†è¶Šæ¥è¶Šå¤šã€‚

ä½¿ç”¨æ–¹å¼è§[example/python/pipelines](./example/python/pipelines),åœ¨è¿™äº›ä»»åŠ¡ç¤ºä¾‹ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº†model apiç¤ºä¾‹æ¥å®ç°åŒæ ·çš„ä»»åŠ¡ã€‚


## æ€§èƒ½

è¯¦ç»†æ€§èƒ½æ•°æ®è¯·ç‚¹å‡»[é“¾æ¥](https://github.com/NetEase-FuXi/EET/blob/main/doc/benchmark.md)æŸ¥çœ‹
* gpt-A100

<div  align="left"> <img src="./doc/image/a100_prompt.png" width = "700" height = "387" alt="a100_prompt"/></div>

* bert-2080ti
<div  align="left"> <img src="./doc/image/bert_ft.png" width = "700" height = "386" alt="bert_ft"/></div>

## Cite Us

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨EETï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰åœ¨æ™ºæºLIVEä¸Šåšåˆ†äº«ï¼Œåˆ†äº«é“¾æ¥ï¼šhttps://event.baai.ac.cn/activities/325

```
@article{eet2022,
  title={Easy and Efficient Transformer : Scalable Inference Solution For large NLP model},
  author={Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao},
  journal={	arXiv:2104.12470},
  year={2022}
}
```
## è”ç³»æˆ‘ä»¬
æ‚¨å¯ä»¥å°†æ‚¨çš„é—®é¢˜å‘å¸ƒåœ¨github issuesï¼Œä¹Ÿå¯ä»¥é€šè¿‡é‚®ä»¶ä¸æˆ‘ä»¬è”ç³»ã€‚

zhaosida@corp.netease.com, zhuangzhong@corp.netease.com, hzzhaozeng@corp.netease.com

å¦‚æœä½ å¯¹é«˜æ€§èƒ½è®¡ç®—ã€æ·±åº¦å­¦ä¹ æ„Ÿå…´è¶£ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬å›¢é˜Ÿï¼Œç®€å†å¯å‘é€ä¸Šè¿°é‚®ç®±ã€‚
